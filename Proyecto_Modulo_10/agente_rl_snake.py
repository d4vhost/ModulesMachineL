# Proyecto_Modulo_10/agente_rl_snake.py

import numpy as np
import random
from collections import defaultdict
import joblib
import os

class AgenteSnakeQL:
    def __init__(self, learning_rate=0.1, discount_factor=0.95, 
                 exploration_rate=1.0, exploration_min=0.01, exploration_decay=0.9995):
        """
        Agente Q-Learning para Snake
        """
        # Q-Table: diccionario de diccionarios
        self.q_table = defaultdict(lambda: defaultdict(float))
        
        # Hiperpar√°metros
        self.alpha = learning_rate          # Tasa de aprendizaje
        self.gamma = discount_factor        # Factor de descuento
        self.epsilon = exploration_rate     # Tasa de exploraci√≥n inicial
        self.epsilon_min = exploration_min  # Exploraci√≥n m√≠nima
        self.epsilon_decay = exploration_decay
        
        # Estad√≠sticas
        self.episodios_entrenados = 0
        self.recompensa_total_historico = []
    
    def elegir_accion(self, estado, acciones_validas):
        """
        Pol√≠tica epsilon-greedy:
        - Exploraci√≥n: acci√≥n aleatoria
        - Explotaci√≥n: mejor acci√≥n conocida
        """
        if random.uniform(0, 1) < self.epsilon:
            # EXPLORACI√ìN: Acci√≥n aleatoria v√°lida
            return random.choice(acciones_validas)
        else:
            # EXPLOTACI√ìN: Mejor acci√≥n seg√∫n Q-Table
            q_valores = [self.q_table[estado][accion] for accion in acciones_validas]
            max_q = max(q_valores)
            
            # Si hay empate, elegir aleatoriamente entre las mejores
            mejores_acciones = [
                acciones_validas[i] for i in range(len(acciones_validas))
                if q_valores[i] == max_q
            ]
            
            return random.choice(mejores_acciones)
    
    def aprender(self, estado, accion, recompensa, siguiente_estado, 
                 acciones_validas_siguiente, game_over):
        """
        Actualizaci√≥n Q-Learning (Ecuaci√≥n de Bellman)
        Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max(Q(s',a')) - Q(s,a)]
        """
        # Valor Q actual
        q_actual = self.q_table[estado][accion]
        
        # Valor Q m√°ximo del siguiente estado
        if game_over or not acciones_validas_siguiente:
            max_q_siguiente = 0.0
        else:
            q_valores_siguiente = [
                self.q_table[siguiente_estado][a] 
                for a in acciones_validas_siguiente
            ]
            max_q_siguiente = max(q_valores_siguiente)
        
        # Actualizaci√≥n Q-Learning
        nuevo_q = q_actual + self.alpha * (
            recompensa + self.gamma * max_q_siguiente - q_actual
        )
        
        self.q_table[estado][accion] = nuevo_q
    
    def reducir_exploracion(self):
        """Reduce epsilon (menos exploraci√≥n con el tiempo)"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def guardar_modelo(self, ruta="models/agente_snake_ql.joblib"):
        """Guarda la Q-Table y configuraci√≥n del agente"""
        os.makedirs(os.path.dirname(ruta), exist_ok=True)
        
        modelo = {
            'q_table': dict(self.q_table),
            'alpha': self.alpha,
            'gamma': self.gamma,
            'epsilon': self.epsilon,
            'episodios': self.episodios_entrenados,
            'historial': self.recompensa_total_historico
        }
        
        joblib.dump(modelo, ruta)
        print(f"‚úÖ Modelo guardado en: {ruta}")
        print(f"   - Estados explorados: {len(self.q_table)}")
        print(f"   - Epsilon actual: {self.epsilon:.4f}")
        print(f"   - Episodios entrenados: {self.episodios_entrenados}")
    
    def cargar_modelo(self, ruta="models/agente_snake_ql.joblib"):
        """Carga la Q-Table y configuraci√≥n"""
        if not os.path.exists(ruta):
            raise FileNotFoundError(f"No se encontr√≥ el modelo en: {ruta}")
        
        modelo = joblib.load(ruta)
        
        # Reconstruir Q-Table
        q_dict = modelo['q_table']
        self.q_table = defaultdict(
            lambda: defaultdict(float),
            {k: defaultdict(float, v) for k, v in q_dict.items()}
        )
        
        self.alpha = modelo.get('alpha', self.alpha)
        self.gamma = modelo.get('gamma', self.gamma)
        self.epsilon = modelo.get('epsilon', self.epsilon_min)
        self.episodios_entrenados = modelo.get('episodios', 0)
        self.recompensa_total_historico = modelo.get('historial', [])
        
        print(f"‚úÖ Modelo cargado desde: {ruta}")
        print(f"   - Estados en memoria: {len(self.q_table)}")
        print(f"   - Episodios previos: {self.episodios_entrenados}")
    
    def modo_juego(self):
        """Configura el agente para jugar (sin exploraci√≥n)"""
        self.epsilon = 0.0
        print("üéÆ Modo juego activado (explotaci√≥n pura)")